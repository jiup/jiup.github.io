---
layout:     post
title:      "网络爬虫背景知识梳理"
subtitle:   "A review on web crawlers"
date:       2016-10-22 17:02:00
author:     "Jiupeng Zhang"
header-img: "img/in-post/coding_1.jpg"
tags:
    - Review
---

# 网络爬虫背景知识梳理

网络爬虫，又称网络机器人，是一种能够自动化浏览网络内容的程序，常作为搜索引擎的数据源，或用于特定领域的数据分析，具有较多的应用场景。

<br/><br/>

## 产生背景

网络爬虫最早出现于1993年，这一年共诞生了*World Wide Web Wanderer*、*Jump Station*、*World Wide Web Worm* 以及 *RBSE spider* 四个爬虫程序，它们主要的工作是通过有限的URL种子集合去收集并统计网络信息。早期网络爬虫通过反复地提取URL并下载Web页面来更新它们的URL仓库。

1994年，两个新的网络爬虫程序：*WebCrawler* 和 *MOM spider* 诞生了，除了收集和统计网络状态以外，它们还首次引入了礼貌爬取和黑名单的概念。此外，*WebCrawler* 能同时处理15个页面链接，被认为是首个可并行化采集的网络爬虫。至此，互联网中被索引的页面的总量从11万页迅速增长到了200万页。在接下来的几年间，像 *Lycos*、*Infoseek*、*Excite*、*AltaVista* 和 *HotBot* 这些商业爬虫也逐渐被世人所知。

到了1998年，*S. Brin 和 L. Page* 为了解决爬虫的可伸缩性，设计了一个大规模网络爬虫，也就是如今大名鼎鼎的 *Google*，它通过多方面来解决网络爬虫的可伸缩性问题：首先，它通过数据压缩和建立索引等一些底层优化来减少磁盘的访问时间；其次，在更高的层面上，*Google* 通过 *PageRank* 算法来计算用户访问页面的几率，这样一来，*Google* 就可以通过不同链接所对应的几率值来对页面进行不同频率的访问，这种方法能通过减少 “冷门” 页面的访问率来显著优化爬虫程序对网络资源的访问。通过使用这一技术，*Google* 实现了对所爬取页面的快速更新。此外，*Google* 使用了采用主-从结构的爬虫集群，其中主机负责向从机分发URL，从机则专门负责对页面进行下载，首版 *Google* 爬虫可达最高100页/秒的页面下载速度。

*Allan Heydon 和 Marc Najork*于1999年实现了一个叫做 *Mercator* 的工具，它尝试解决了网络爬虫的可扩展问题，它通过利用基于 *Java* 的模块化框架开发的优势，允许第三方组件与 *Mercator* 进行集成。为了进一步解决了可伸缩问题，*Mercator* 引入了URL去重（URL-Seen）概念，通过不再访问已访问过的页面来进一步扩大爬虫的搜索范围，然而，这个看似简单的去重操作却随着URL记录的增加而变得非常耗时，于是，*Mercator* 改进后采用了RAM缓存与批量磁盘检查相补充的机制进行URL的去重。使用了此技术的第二版 *Mercator* 爬虫最终爬取了高达8.91亿个网页。

网络爬虫技术一直在不断的发展，*IBM* 的 *WebFountain* 能够保持高更新率对网页进行整体离线，*UbiCrawler* 通过P2P技术将网页去重的工作分担给了每个用户，在5台1GHZ的计算机，单机并行50个线程的条件下，达到了1000万页/天的下载速度，2008年，一个叫 *IRL-bot* 的特大规模网络爬虫在甚至在一台主频为2.6GHZ的双核计算机连续运行41.27天的情况下爬取了多达63.8亿的网页数据。

当然，网络爬虫系统追求的不仅仅是效率与规模，在互联网技术蓬勃发展的今天，对网站进行性能测试或漏洞检测，对网页文档对象模型（DOM）进行结构化分析，对特定内容进行主题爬取，对深度网络空间进行探索，以及对富网络应用（RIA）数据的精确抽取等也为网络爬虫系统的研究与发展带来了新的挑战与机遇。

<br/><br/>

## 常见分类

#### 通用网络爬虫（Scalable Web Crawler）

又称全网爬虫，爬行对象从指定的种子 URL集合开始，随后逐渐扩充到整个网络空间，该类爬虫程序的服务对象主要为搜索引擎，或一些大型的Web数据提供商。这类网络爬虫的爬行范围和数量巨大，虽然对于爬行页面的时效要求相对较低，但它对于爬行速率和存储容量却要求很高。这类爬虫通常使用并行的工作方式，其处理页面的范围十分广泛，这也使得该类爬虫很难保证网络内容的及时更新。虽然存在一些不足，但通用网络爬虫常常能为搜索引擎提供广泛的主题和数据来源，因此具有较强的应用价值。

#### 聚焦网络爬虫（Focused Web Crawler）

此类爬虫又称主题爬虫（Topical Crawler），是能选择性爬取那些与预定义主题高度相关的内容的网络爬虫（如新闻爬虫，论坛爬虫，产品爬虫等），此类爬虫往往周期性的访问一些高时效数据，应用广泛，爬取目标专一，能够很好地满足一些特定人群对特定领域信息的需求。

#### 增量网络爬虫（Incremental Web Crawler）

是对已下载网页采取增量式更新并只爬行更新内容的爬虫，着重追求提高所爬取内容的新鲜度。和周期性爬行并更新页面的网络爬虫相比，增量式爬虫只在必要的时刻爬行那些新发布的或发生更新的页面。它并不对没有兴趣，或者未达到更新周期的页面进行二次下载，因此该类爬虫能够更精准的获取信息的变化，并能节省资源，减少时间和空间上的浪费。

#### 深层网络爬虫

又称暗网（Deep Web）或隐藏网络（Hidden Web）爬虫，专负责获取搜索引擎无法索引的、超链接不可达的或需提交表单（如需登录或详细配置）后才可见的网络内容。2000年，*Bright Planet* 曾指出，深层网络中可访问的信息容量是表层网络的几百倍，可见深层网络中内容资源之多，隐含的数据价值之大。

<br/><br/>

## 基本流程

网络爬虫（也称网络蜘蛛），简单来说就是一个可按照指定规则，自动化浏览网络内容的机器人程序，其最基本的工作原理如下图所示：

![image-20190302164925858](https://github.com/jiup/jiup.github.io/raw/master/img/in-post/image-20190302164925858.png)

给定一初始URL集合，将其加入到任务队列中，程序便准备就绪了。爬虫运行时，按照特定的顺序依次访问队列中的URL/组。接下来，一方面对感兴趣的内容进行筛选及持久化，一边提取网页中的外链，将新的URL集合进行筛选和调整，然后加回到任务队列中等待下一轮爬取，直到满足特定终止条件时，便终止程序。

从上图可大致看出，爬虫程序本身的运行是从任务队列到页面处理，再将页面内链接自动反馈回任务队列，这样一个循环往复的过程，由此，无需人工干预，网络爬虫程序便能够自主地遍历网络资源。

<br/><br/>

## 发展趋势

#### 分布式爬虫

一个高性能的单机爬虫在理想状态下能够利用该机所处网络环境的全部带宽，而这正是单机爬虫系统不可逾越的性能瓶颈，为了最大化爬虫的访问性能，分布式爬虫应运而生。此类爬虫系统除了能进一步扩大系统整体网络带宽外，还能够充分利用对端CDN的本地化优势进一步提高访问效率。但是，分布式系统所带来的同步开销又为此类爬虫程序引入了更多复杂因素。

#### 富网络应用爬虫

随着通信技术的浏览器的不断发展，富网络应用给传统网络爬虫带来了诸多难题。首先，传统网络爬虫的理论模型建立在以网页为节点，网页中的链接为边的有向图模型之上，这种模型默认每个URL对应着相互独立的网络内容，而目前越来越多的网站采用了包括AJAX、HTML5在内的动态内容加载技术，这类新的Web内容大大提高了用户体验，但与此同时，它却使得传统网络爬虫无法准确处理，甚至根本无法识别这些数据。

富网络应用爬虫（Rich Internet Application）由此浮出水面，此类爬虫程序除了抽取传统HTML页面中的超链接外，还可以通过静态分析与网页关联的动态脚本（如 JavaScript）来提取更多的目标链接；更有一些爬虫将无头浏览器嵌入了爬虫系统，使其能够完整获取还原度极高的网络内容，但由于其加载了很多不必要的资源，如果不定制开发的话，很难拥有理想的数据刮取性能。此外，除了对目标链接的提取，一些网络应用在不同的操作逻辑下会显示不同的内容（如前端动态构造的URL），如何处理这些内容，并将当前状态标定为独立的资源成为了新的难题。总之，现阶段的富网络应用爬虫程序大多都是针对特定的目标而定制化编写的。

#### 智能爬虫

智能爬虫通过采用机器学习方法，对网页内容进行分类，并能预测网站的更新周期。利用主题敏感模型，可使爬虫高质量的预判网络链接与目标的相关度，进而有效提高网络爬虫系统的整体效果。此外，在爬虫任务调度模块中引入恰当的启发式算法，更加灵活地对待爬任务集合进行整理和优化，能够显著提高网络爬虫系统的工作质量。

<br/><br/>

## 网路爬虫技术点

##### 爬行策略（最佳优先）

##### 链接去重（哈希，布隆过滤器）

<br/><br/>

## 反爬虫技术

现实中的网络爬虫程序良莠不齐，一些网络爬虫程序频繁的访问同一站点，会给对方网站服务器带来巨大的压力。由此一来，网站管理员有必要对那些 “不文明” 的网络爬虫采取一些反制措施，以确保真实用户的正常访问，网站管理员和爬虫开发者之间的技术较量便开始了。

#### Robots协议

Robots.txt，以文本形式存储在服务器根目录下，用来声明哪些爬虫受网站欢迎（通过设置爬虫特定的UA），哪些爬虫不受欢迎；网站中哪些内容应该被爬虫程序所访问，哪些内容不应该被访问等。如今，一些改良的Robots协议还支持网站对其希望被访问的频率、站点地图等信息进行声明。需要注意的是，Robots协议只能算是一种“君子协定”，它只能声明网站的诉求，但并不能真正限制网络爬虫程序的行为。

#### IP地址和XFF属性

一些网络爬虫会不加限制地频繁访问同一站点，这会造成该站点的服务器承受过高的访问压力。为防止此类事故发生，网络管理员通常会对此类来访IP地址采用封禁或流量管控等手段来限制爬虫程序的访问。

当网络爬虫使用HTTP正向代理对网络请求进行间接访问时，代理方为了标记连接发起的原始IP，会在HTTP请求头中添加 *XFF*（X-Forwarded-For）属性，通过个属性，网络管理员便可能得知发出请求的源IP地址。但是由于HTTP请求是由请求方自己构建的，请求方可以对此字段任意修改，通过该字段进行封禁并不可靠。可见*XFF*的有效性取决于代理服务器所提供原始IP地址的真实性。

#### User-Agent

为了进行访问统计，追踪一些异常通信或者区分用户的类型，HTTP协议在请求头中加入了UA字段。网站管理员可以通过服务器端建立黑名单的方式，对一些恶意爬虫进行临时/永久封禁。该字段和 *XFF* 属性类似，它无法得知请求方是否对此字段进行过伪造，所以这种封禁方法也不完全可靠。

#### 验证码

验证码为遏制猖獗的恶意爬虫而生，事实上，以通过验证作为提供内容前提的这套逻辑有效地阻止了绝大多数的爬虫程序。但随着开发人员的技术提高，一些验证码已经远不能提供预期的安全保障，网站主不得不开发出更复杂的验证码来阻止这番攻势，使得很多真实用户也很难通过验证，是便是某些验证码被大家诟病的原因。如今，合理的验证码设计，加上风险分级，频率控制，短信双重验证，开放爬虫API等手段，能达到保障用户体验的前提下，有效阻止恶意爬虫的目的。

<br/><br/>

## 网页信息提取

网页信息提取，是网络爬虫系统处理网络内容的关键阶段。通常，网页内容包含在HTML页面的DOM中，对DOM内容的抽取，通常有以下手段：

#### 正则表达式

正则表达式是一种通过描述文本规则，对文本内容进行检索、验证和替换的有力工具。由于DOM以树形结构组织，构造选取HTML文档中某一元素的正则表达式非常麻烦，尤其在DOM树很深的情况下性能较差。但当程序仅仅对信息进行校验，或对节点内文本进行二次抽取时，使用正则表达式则会体现出更多性能优势。

#### XML路径语言

*XPath*，全称 XML 路径语言（XML Path Language），用来对XML文档中的数据进行定位。XPath语法简洁，并具有在数据结构树中找寻节点的能力，可用于对DOM的全局处理。由于XML是HTML的超集，HTML内容基本满足XML文件格式，因此，它可以用来对HTML文档中的元素或属性进行访问。例如，使用XPath在DOM中提取全部`a`标签所对应的链接，使用 `//a/@href` 即可，非常简洁易用。

<br/><br/>

## 动态内容获取

目前越来越多的网站使用了包括AJAX、HTML5在内的动态内容加载技术，这种新的Web设计方式提高了网络应用的响应速度以及可交互性，但与此同时，这些将一些重要内容通过JavaScript等前端脚本语言加载的方式，使得网络爬虫系统无法通过直接分析HTML文档来获得这些重要内容，而必须对这个加载网页的脚本进行分析。某些特定的爬虫，为了增强其获取动态页面内容的能力，使用 *PhantomJS* 或 *Selenium* 来获取动态页面的内容。*PhantomJS* 是一个Headless WebKit引擎，它可和浏览器一样布局和渲染页面。通过它，网络爬虫程序便能获取到渲染后的动态网页内容，当然，*PhantomJS* 对页面脚本的执行和渲染会带来不小的性能开销。总之，对于网络爬虫系统而言，支持动态页面内容的获取，也算是进一步提升了爬虫的爬行范围。

<br/><br/>

本文所有权归作者所有，禁止未经许可的转载，违者将追究相关责任。

原始链接：<https://jiup.github.io/2016/10/22/web-crawlers/>